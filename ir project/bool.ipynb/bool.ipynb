{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               *****    Documents in  Terms   *****                                                                        \n",
      "     [['antony', 'brutus', 'caeser', 'cleopatra', 'mercy', 'worser'], ['antony', 'brutus', 'caeser', 'calpurnia'], ['mercy', 'worser'], ['brutus', 'caeser', 'mercy', 'worser'], ['caeser', 'mercy', 'worser'], ['antony', 'caeser', 'mercy'], ['angels', 'fools', 'fear', 'in', 'rush', 'to', 'tread', 'where'], ['angels', 'fools', 'fear', 'in', 'rush', 'to', 'tread', 'where'], ['angels', 'fools', 'in', 'rush', 'to', 'tread', 'where'], ['fools', 'fear', 'in', 'rush', 'to', 'tread', 'where']] \n",
      "\n",
      "                                               *****     Stemmed Terms   *****                                                                        \n",
      "     [['antoni'], ['brutu'], ['caeser'], ['cleopatra'], ['merci'], ['worser'], ['antoni'], ['brutu'], ['caeser'], ['calpurnia'], ['merci'], ['worser'], ['brutu'], ['caeser'], ['merci'], ['worser'], ['caeser'], ['merci'], ['worser'], ['antoni'], ['caeser'], ['merci'], ['angel'], ['fool'], ['fear'], ['in'], ['rush'], ['to'], ['tread'], ['where'], ['angel'], ['fool'], ['fear'], ['in'], ['rush'], ['to'], ['tread'], ['where'], ['angel'], ['fool'], ['in'], ['rush'], ['to'], ['tread'], ['where'], ['fool'], ['fear'], ['in'], ['rush'], ['to'], ['tread'], ['where']] \n",
      "\n",
      "\n",
      "                                               *****     positions    *****                                       \n",
      "   \n",
      "{'antony': [3, {0: [0], 1: [0], 5: [0]}], 'brutus': [3, {0: [1], 1: [1], 3: [0]}], 'caeser': [5, {0: [2], 1: [2], 3: [1], 4: [0], 5: [1]}], 'cleopatra': [1, {0: [3]}], 'mercy': [5, {0: [4], 2: [0], 3: [2], 4: [1], 5: [2]}], 'worser': [4, {0: [5], 2: [1], 3: [3], 4: [2]}], 'calpurnia': [1, {1: [3]}], 'angels': [3, {6: [0], 7: [0], 8: [0]}], 'fools': [4, {6: [1], 7: [1], 8: [1], 9: [0]}], 'fear': [3, {6: [2], 7: [2], 9: [1]}], 'in': [4, {6: [3], 7: [3], 8: [2], 9: [2]}], 'rush': [4, {6: [4], 7: [4], 8: [3], 9: [3]}], 'to': [4, {6: [5], 7: [5], 8: [4], 9: [4]}], 'tread': [4, {6: [6], 7: [6], 8: [5], 9: [5]}], 'where': [4, {6: [7], 7: [7], 8: [6], 9: [6]}]} \n",
      "\n",
      "                                    ***** all response list *****                                                     \n",
      "[[0, 1], [0, 1], [], [0], [], [0], [], [], [], []] \n",
      "\n",
      "                                  *****response list to query*****                                              \n",
      "1 [0, 1]\n",
      "2 [0, 1]\n",
      "                                  *****term freq for each  term in doc*****                                              \n",
      "\n",
      "           doc1  doc2  doc3  doc4  doc5  doc6  doc7  doc8  doc9  doc10\n",
      "antony        1     1     0     0     0     1     0     0     0      0\n",
      "brutus        1     1     0     1     0     0     0     0     0      0\n",
      "caeser        1     1     0     1     1     1     0     0     0      0\n",
      "cleopatra     1     0     0     0     0     0     0     0     0      0\n",
      "mercy         1     0     1     1     1     1     0     0     0      0\n",
      "worser        1     0     1     1     1     0     0     0     0      0\n",
      "calpurnia     0     1     0     0     0     0     0     0     0      0\n",
      "angels        0     0     0     0     0     0     1     1     1      0\n",
      "fools         0     0     0     0     0     0     1     1     1      1\n",
      "fear          0     0     0     0     0     0     1     1     0      1\n",
      "in            0     0     0     0     0     0     1     1     1      1\n",
      "rush          0     0     0     0     0     0     1     1     1      1\n",
      "to            0     0     0     0     0     0     1     1     1      1\n",
      "tread         0     0     0     0     0     0     1     1     1      1\n",
      "where         0     0     0     0     0     0     1     1     1      1\n",
      "                                  ***** weighted term freq for each  term in doc*****                                              \n",
      "\n",
      "           doc1  doc2  doc3  doc4  doc5  doc6  doc7  doc8  doc9  doc10\n",
      "antony      1.0   1.0   0.0   0.0   0.0   1.0   0.0   0.0   0.0    0.0\n",
      "brutus      1.0   1.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0    0.0\n",
      "caeser      1.0   1.0   0.0   1.0   1.0   1.0   0.0   0.0   0.0    0.0\n",
      "cleopatra   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    0.0\n",
      "mercy       1.0   0.0   1.0   1.0   1.0   1.0   0.0   0.0   0.0    0.0\n",
      "worser      1.0   0.0   1.0   1.0   1.0   0.0   0.0   0.0   0.0    0.0\n",
      "calpurnia   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0    0.0\n",
      "angels      0.0   0.0   0.0   0.0   0.0   0.0   1.0   1.0   1.0    0.0\n",
      "fools       0.0   0.0   0.0   0.0   0.0   0.0   1.0   1.0   1.0    1.0\n",
      "fear        0.0   0.0   0.0   0.0   0.0   0.0   1.0   1.0   0.0    1.0\n",
      "in          0.0   0.0   0.0   0.0   0.0   0.0   1.0   1.0   1.0    1.0\n",
      "rush        0.0   0.0   0.0   0.0   0.0   0.0   1.0   1.0   1.0    1.0\n",
      "to          0.0   0.0   0.0   0.0   0.0   0.0   1.0   1.0   1.0    1.0\n",
      "tread       0.0   0.0   0.0   0.0   0.0   0.0   1.0   1.0   1.0    1.0\n",
      "where       0.0   0.0   0.0   0.0   0.0   0.0   1.0   1.0   1.0    1.0 \n",
      "\n",
      "\n",
      "                                  *****        IDF          *****                                              \n",
      "            df       idf\n",
      "antony     3.0  0.522879\n",
      "brutus     3.0  0.522879\n",
      "caeser     5.0   0.30103\n",
      "cleopatra  1.0       1.0\n",
      "mercy      5.0   0.30103\n",
      "worser     4.0   0.39794\n",
      "calpurnia  1.0       1.0\n",
      "angels     3.0  0.522879\n",
      "fools      4.0   0.39794\n",
      "fear       3.0  0.522879\n",
      "in         4.0   0.39794\n",
      "rush       4.0   0.39794\n",
      "to         4.0   0.39794\n",
      "tread      4.0   0.39794\n",
      "where      4.0   0.39794\n",
      "                                  *****         TF*IDF             *****                                              \n",
      "\n",
      "               doc1      doc2     doc3      doc4     doc5      doc6      doc7  \\\n",
      "antony     0.522879  0.522879      0.0       0.0      0.0  0.522879       0.0   \n",
      "brutus     0.522879  0.522879      0.0  0.522879      0.0       0.0       0.0   \n",
      "caeser      0.30103   0.30103      0.0   0.30103  0.30103   0.30103       0.0   \n",
      "cleopatra       1.0       0.0      0.0       0.0      0.0       0.0       0.0   \n",
      "mercy       0.30103       0.0  0.30103   0.30103  0.30103   0.30103       0.0   \n",
      "worser      0.39794       0.0  0.39794   0.39794  0.39794       0.0       0.0   \n",
      "calpurnia       0.0       1.0      0.0       0.0      0.0       0.0       0.0   \n",
      "angels          0.0       0.0      0.0       0.0      0.0       0.0  0.522879   \n",
      "fools           0.0       0.0      0.0       0.0      0.0       0.0   0.39794   \n",
      "fear            0.0       0.0      0.0       0.0      0.0       0.0  0.522879   \n",
      "in              0.0       0.0      0.0       0.0      0.0       0.0   0.39794   \n",
      "rush            0.0       0.0      0.0       0.0      0.0       0.0   0.39794   \n",
      "to              0.0       0.0      0.0       0.0      0.0       0.0   0.39794   \n",
      "tread           0.0       0.0      0.0       0.0      0.0       0.0   0.39794   \n",
      "where           0.0       0.0      0.0       0.0      0.0       0.0   0.39794   \n",
      "\n",
      "               doc8      doc9     doc10  \n",
      "antony          0.0       0.0       0.0  \n",
      "brutus          0.0       0.0       0.0  \n",
      "caeser          0.0       0.0       0.0  \n",
      "cleopatra       0.0       0.0       0.0  \n",
      "mercy           0.0       0.0       0.0  \n",
      "worser          0.0       0.0       0.0  \n",
      "calpurnia       0.0       0.0       0.0  \n",
      "angels     0.522879  0.522879       0.0  \n",
      "fools       0.39794   0.39794   0.39794  \n",
      "fear       0.522879       0.0  0.522879  \n",
      "in          0.39794   0.39794   0.39794  \n",
      "rush        0.39794   0.39794   0.39794  \n",
      "to          0.39794   0.39794   0.39794  \n",
      "tread       0.39794   0.39794   0.39794  \n",
      "where       0.39794   0.39794   0.39794  \n",
      "                                  *****         Doc Length             *****                                              \n",
      "        doc1__length  doc2__length  doc3__length  doc4__length  doc5__length  \\\n",
      "length      1.373462      1.279618      0.498974      0.782941      0.582747   \n",
      "\n",
      "        doc6__length  doc7__length  doc8__length  doc9__length  doc10__length  \n",
      "length       0.67427      1.223496      1.223496      1.106137       1.106137   \n",
      "\n",
      "                                  *****        Normalized TF*IDF           *****                                              \n",
      "\n",
      "               doc1      doc2      doc3      doc4      doc5      doc6  \\\n",
      "antony     0.380701  0.408621       0.0       0.0       0.0  0.775474   \n",
      "brutus     0.380701  0.408621       0.0  0.667839       0.0       0.0   \n",
      "caeser     0.219176   0.23525       0.0  0.384486   0.51657  0.446453   \n",
      "cleopatra  0.728087       0.0       0.0       0.0       0.0       0.0   \n",
      "mercy      0.219176       0.0  0.603298  0.384486   0.51657  0.446453   \n",
      "worser     0.289735       0.0  0.797516  0.508263  0.682869       0.0   \n",
      "calpurnia       0.0  0.781483       0.0       0.0       0.0       0.0   \n",
      "angels          0.0       0.0       0.0       0.0       0.0       0.0   \n",
      "fools           0.0       0.0       0.0       0.0       0.0       0.0   \n",
      "fear            0.0       0.0       0.0       0.0       0.0       0.0   \n",
      "in              0.0       0.0       0.0       0.0       0.0       0.0   \n",
      "rush            0.0       0.0       0.0       0.0       0.0       0.0   \n",
      "to              0.0       0.0       0.0       0.0       0.0       0.0   \n",
      "tread           0.0       0.0       0.0       0.0       0.0       0.0   \n",
      "where           0.0       0.0       0.0       0.0       0.0       0.0   \n",
      "\n",
      "               doc7      doc8      doc9     doc10  \n",
      "antony          0.0       0.0       0.0       0.0  \n",
      "brutus          0.0       0.0       0.0       0.0  \n",
      "caeser          0.0       0.0       0.0       0.0  \n",
      "cleopatra       0.0       0.0       0.0       0.0  \n",
      "mercy           0.0       0.0       0.0       0.0  \n",
      "worser          0.0       0.0       0.0       0.0  \n",
      "calpurnia       0.0       0.0       0.0       0.0  \n",
      "angels     0.427365  0.427365  0.472707       0.0  \n",
      "fools      0.325248  0.325248  0.359756  0.359756  \n",
      "fear       0.427365  0.427365       0.0  0.472707  \n",
      "in         0.325248  0.325248  0.359756  0.359756  \n",
      "rush       0.325248  0.325248  0.359756  0.359756  \n",
      "to         0.325248  0.325248  0.359756  0.359756  \n",
      "tread      0.325248  0.325248  0.359756  0.359756  \n",
      "where      0.325248  0.325248  0.359756  0.359756  \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 203\u001b[0m\n\u001b[0;32m    200\u001b[0m query_parts \u001b[38;5;241m=\u001b[39m query\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39msplit(boolean_operator)\n\u001b[0;32m    201\u001b[0m \u001b[38;5;66;03m# Tokenization and stemming for each part\u001b[39;00m\n\u001b[0;32m    202\u001b[0m query_parts_terms \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 203\u001b[0m     [PorterStemmer\u001b[38;5;241m.\u001b[39mstem(terms) \u001b[38;5;28;01mfor\u001b[39;00m term \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdocument_of_terms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mterm\u001b[49m\u001b[43m)\u001b[49m] \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m query_parts\n\u001b[0;32m    204\u001b[0m ]\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28mprint\u001b[39m(query_parts_terms)\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m boolean_operator:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;66;03m# Find matched documents for each part\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from natsort import natsorted\n",
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "#preprocessing\n",
    "       \n",
    "files_name=natsorted(os.listdir('files'))\n",
    "document_of_terms=[]\n",
    "for files in files_name:\n",
    "    with open(f'files/{files}','r') as f:\n",
    "        document=f.read()\n",
    "        \n",
    "         \n",
    "    tokenized_documents=word_tokenize(document)\n",
    "    terms=[]\n",
    "    for word in tokenized_documents:\n",
    "            terms.append(word)\n",
    "    document_of_terms.append(terms)\n",
    "\n",
    "print('                                               *****    Documents in  Terms   *****                                                                        \\n    '\n",
    "    ,  document_of_terms,'\\n')                        \n",
    "\n",
    "#stemming \n",
    "stemmer=PorterStemmer()\n",
    "document_of_stemms=[]\n",
    "for terms in document_of_terms:\n",
    "     for word in terms :\n",
    "         stemmed_terms=[stemmer.stem(word)]\n",
    "         document_of_stemms.append(stemmed_terms)\n",
    "\n",
    "print('                                               *****     Stemmed Terms   *****                                                                        \\n    '\n",
    "    ,  document_of_stemms,'\\n')        \n",
    "\n",
    "#positional_index\n",
    "\n",
    "document_number = 0\n",
    "positional_index = {}\n",
    "\n",
    "print() \n",
    "print(\"                                               *****     positions    *****                                       \\n   \")\n",
    "for document in document_of_terms:\n",
    "    for positional, term in enumerate(document):\n",
    "        if term in positional_index:\n",
    "            positional_index[term][0] = positional_index[term][0] + 1\n",
    "\n",
    "            if document_number in positional_index[term][1]:\n",
    "                positional_index[term][1][document_number].append(positional)\n",
    "            else:\n",
    "                positional_index[term][1][document_number] = [positional]\n",
    "\n",
    "        else:\n",
    "            positional_index[term] = []\n",
    "            positional_index[term].append(1)\n",
    "            positional_index[term].append({})\n",
    "            positional_index[term][1][document_number] = [positional]\n",
    "\n",
    "    document_number += 1\n",
    "    \n",
    "\n",
    "print(positional_index,'\\n')\n",
    "\n",
    "\n",
    "#query_preprocessing\n",
    "\n",
    "query=input('put query here:')\n",
    "\n",
    "final_list=[[]for i in range (10)]\n",
    "print('                                    ***** all response list *****                                                     ')\n",
    "for word in query.split():    \n",
    "    if word in positional_index.keys():\n",
    "        for key in positional_index[word][1].keys():\n",
    "            \n",
    "            if final_list[key]!=[]:\n",
    "                \n",
    "                 if final_list[key][-1] == positional_index[word][1][key][0]-1:\n",
    "                        final_list[key].append(positional_index[word][1][key][0])\n",
    "        \n",
    "            else:\n",
    "                 final_list[key].append(positional_index[word][1][key][0])\n",
    "            \n",
    "print(final_list,'\\n')\n",
    "\n",
    "print('                                  *****response list to query*****                                              ')\n",
    "for position,list in enumerate(final_list,start=1):\n",
    "    if len(list)==len(query.split()):\n",
    "      print(position,list)     \n",
    "\n",
    "\n",
    "\n",
    "#find tf and wtf\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "\n",
    "all_words=[]\n",
    "for doc in document_of_terms:\n",
    "    for word in doc:\n",
    "        all_words.append(word)\n",
    " #tf       \n",
    "def get_term_freq(doc):\n",
    "    words_found=dict.fromkeys(all_words,0)\n",
    "    for word in doc:\n",
    "        words_found[word]+=1\n",
    "    return words_found\n",
    "\n",
    "term_freq=pd.DataFrame(get_term_freq(document_of_terms[0]).values(),index=get_term_freq(document_of_terms[0]).keys())              \n",
    "print('                                  *****term freq for each  term in doc*****                                              ')\n",
    "print()        \n",
    "for i in range(1,len(document_of_terms)):\n",
    "    term_freq[i]=get_term_freq(document_of_terms[i]).values()\n",
    "    \n",
    "term_freq.columns=['doc'+str(i) for i in range (1,11)]    \n",
    "print(term_freq)     \n",
    "\n",
    "#wtf\n",
    "\n",
    "def get_weighted_term_freq(x):\n",
    "    if x>0:\n",
    "      return math.log(x)+1\n",
    "    return 0\n",
    "     \n",
    "print('                                  ***** weighted term freq for each  term in doc*****                                              ')\n",
    "print()        \n",
    "for i in range(1,len(document_of_terms)+1):\n",
    "    term_freq['doc'+str(i)]= term_freq['doc'+str(i)].apply(get_weighted_term_freq) \n",
    "    \n",
    "print(term_freq,'\\n')\n",
    "print()\n",
    " \n",
    "#idf and tf*idf\n",
    " \n",
    "tfd=pd.DataFrame(columns=['df','idf'])\n",
    "\n",
    "print('                                  *****        IDF          *****                                              ')\n",
    "for i in range(len(term_freq)):\n",
    "    \n",
    "    frequancy=term_freq.iloc[i].values.sum()\n",
    "    \n",
    "    tfd.loc[i,'df']=frequancy\n",
    "    tfd.loc[i,'idf']=math.log10(10/(float(frequancy)))\n",
    "    \n",
    "tfd.index=term_freq.index\n",
    "    \n",
    "print(tfd)\n",
    "\n",
    "# tf*idf\n",
    "\n",
    "tf_idf=term_freq.multiply(tfd['idf'],axis=0)\n",
    "\n",
    "print('                                  *****         TF*IDF             *****                                              ')\n",
    "print()      \n",
    "print(tf_idf)\n",
    "\n",
    "\n",
    "# doc length\n",
    "\n",
    "def get_doc_len(col):\n",
    "    return np.sqrt(tf_idf[col].apply(lambda x:x**2).sum())\n",
    "\n",
    "doc_len=pd.DataFrame()\n",
    "for col in tf_idf.columns:\n",
    "    doc_len.loc['length', col+'__length']=get_doc_len(col)   \n",
    "print('                                  *****         Doc Length             *****                                              ') \n",
    "print(doc_len,'\\n')    \n",
    "\n",
    "\n",
    "\n",
    "# normalization\n",
    "norm_tf_idf= tf_idf.divide(doc_len.values[0],axis=1)\n",
    "print('                                  *****        Normalized TF*IDF           *****                                              ')\n",
    "print()\n",
    "print(norm_tf_idf)\n",
    "\n",
    "\n",
    "# Handling boolean operators\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "files_name=natsorted(os.listdir('files'))\n",
    "document_of_terms=[]\n",
    "for files in files_name:\n",
    "    with open(f'files/{files}','r') as f:\n",
    "        document=f.read()\n",
    "        \n",
    "boolean_operators = ['and', 'or', 'not']\n",
    "boolean_operator = None\n",
    "\n",
    "for op in boolean_operators:\n",
    "    if op in document_of_terms:\n",
    "        boolean_operator = op\n",
    "        break\n",
    "\n",
    "matched_documents = None\n",
    "\n",
    "# Split the query based on the boolean operator\n",
    "query_parts = query.lower().split(boolean_operator)\n",
    "# Tokenization and stemming for each part\n",
    "query_parts_terms = [\n",
    "    [PorterStemmer.stem(terms) for term in document_of_terms(term)] for part in query_parts\n",
    "]\n",
    "print(query_parts_terms)\n",
    "\n",
    "if boolean_operator:\n",
    "    # Find matched documents for each part\n",
    "    matched_documents_parts = []\n",
    "    for part_terms in query_parts_terms:\n",
    "        matched_documents_part = set(range(1, len(document_lengths) + 1))  # Initialize with all documents\n",
    "        for term in part_terms:\n",
    "            if term in positional_index:\n",
    "                matched_documents_part &= set(positional_index[term]['docs'].keys()) # type: ignore\n",
    "        matched_documents_parts.append(matched_documents_part)\n",
    "    print(matched_documents_parts)\n",
    "    # Apply boolean operator\n",
    "    if boolean_operator == 'and':\n",
    "        matched_documents = set.intersection(*matched_documents_parts)\n",
    "    elif boolean_operator == 'or':\n",
    "        matched_documents = set.union(*matched_documents_parts)\n",
    "    elif boolean_operator == 'not':\n",
    "        matched_documents = set.difference(set(range(1, len(document_lengths) + 1)), *matched_documents_parts)\n",
    "    \n",
    "else:\n",
    "    # No boolean operator, find matched documents without considering boolean logic\n",
    "    matched_documents = set(range(1, len(document_lengths) + 1))\n",
    "    for term in document_of_terms:\n",
    "        if term in positional_index:\n",
    "            matched_documents &= set(positional_index[term]['docs'].keys()) # type: ignore\n",
    "\n",
    "print(\"Matched Documents:\", matched_documents)\n",
    "\n",
    "\n",
    "# Get distinct terms in the query\n",
    "document_of_terms = [item for sublist in query_parts_terms for item in sublist]\n",
    "original_document_of_terms = document_of_terms\n",
    "document_of_terms = list(set(document_of_terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# edit in code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in query:\n",
    "    if word.lower() != \"and\" and word.lower() != \"or\" and word.lower() != \"not\":\n",
    "        different_words.append(word.lower())\n",
    "    else:\n",
    "        connecting_words.append(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from nltk.tokenize import word_tokenize\n",
    "from natsort import natsorted\n",
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "#preprocessing\n",
    "       \n",
    "files_name=natsorted(os.listdir('Files'))\n",
    "document_of_terms=[]\n",
    "for files in files_name:\n",
    "    with open(f'Files/{files}','r') as f:\n",
    "        document=f.read()\n",
    "        \n",
    "         \n",
    "    tokenized_documents=word_tokenize(document)\n",
    "    terms=[]\n",
    "    for word in tokenized_documents:\n",
    "            terms.append(word)\n",
    "    document_of_terms.append(terms)\n",
    "\n",
    "print('                                               ***    Documents in  Terms   ***                                                                        \\n    '\n",
    "    ,  document_of_terms,'\\n')                        \n",
    "\n",
    "#stemming \n",
    "stemmer=PorterStemmer()\n",
    "document_of_stemms=[]\n",
    "for terms in document_of_terms:\n",
    "     for word in terms :\n",
    "         stemmed_terms=[stemmer.stem(word)]\n",
    "         document_of_stemms.append(stemmed_terms)\n",
    "\n",
    "print('                                               ***     Stemmed Terms   ***                                                                        \\n    '\n",
    "    ,  document_of_stemms,'\\n')        \n",
    "\n",
    "#positional_index\n",
    "\n",
    "document_number = 0\n",
    "positional_index = {}\n",
    "\n",
    "print() \n",
    "print(\"                                               ***     positions    ***                                       \\n   \")\n",
    "for document in document_of_terms:\n",
    "    for positional, term in enumerate(document):\n",
    "        if term in positional_index:\n",
    "            positional_index[term][0] = positional_index[term][0] + 1\n",
    "\n",
    "            if document_number in positional_index[term][1]:\n",
    "                positional_index[term][1][document_number].append(positional)\n",
    "            else:\n",
    "                positional_index[term][1][document_number] = [positional]\n",
    "\n",
    "        else:\n",
    "            positional_index[term] = []\n",
    "            positional_index[term].append(1)\n",
    "            positional_index[term].append({})\n",
    "            positional_index[term][1][document_number] = [positional]\n",
    "\n",
    "    document_number += 1\n",
    "    \n",
    "\n",
    "print(positional_index,'\\n')\n",
    "\n",
    "\n",
    "#query_preprocessing\n",
    "\n",
    "query=input('put query here:')\n",
    "\n",
    "final_list=[[]for i in range (10)]\n",
    "print('                                    *** all response list ***                                                     ')\n",
    "for word in query.split():    \n",
    "    if word in positional_index.keys():\n",
    "        for key in positional_index[word][1].keys():\n",
    "            \n",
    "            if final_list[key]!=[]:\n",
    "                \n",
    "                 if final_list[key][-1] == positional_index[word][1][key][0]-1:\n",
    "                        final_list[key].append(positional_index[word][1][key][0])\n",
    "        \n",
    "            else:\n",
    "                 final_list[key].append(positional_index[word][1][key][0])\n",
    "            \n",
    "print(final_list,'\\n')\n",
    "\n",
    "print('                                  ****response list to query****                                              ')\n",
    "for position,list in enumerate(final_list,start=1):\n",
    "    if len(list)==len(query.split()):\n",
    "      print(position,list)     \n",
    "\n",
    "\n",
    "\n",
    "#find tf and wtf\n",
    "\n",
    "\n",
    "\n",
    "all_words=[]\n",
    "for doc in document_of_terms:\n",
    "    for word in doc:\n",
    "        all_words.append(word)\n",
    " #tf       \n",
    "def get_term_freq(doc):\n",
    "    words_found=dict.fromkeys(all_words,0)\n",
    "    for word in doc:\n",
    "        words_found[word]+=1\n",
    "    return words_found\n",
    "\n",
    "term_freq=pd.DataFrame(get_term_freq(document_of_terms[0]).values(),index=get_term_freq(document_of_terms[0]).keys())              \n",
    "print('                                  ****term freq for each  term in doc****                                              ')\n",
    "print()        \n",
    "for i in range(1,len(document_of_terms)):\n",
    "    term_freq[i]=get_term_freq(document_of_terms[i]).values()\n",
    "    \n",
    "term_freq.columns=['doc'+str(i) for i in range (1,12)]    \n",
    "print(term_freq)     \n",
    "\n",
    "#wtf\n",
    "\n",
    "def get_weighted_term_freq(x):\n",
    "    if x>0:\n",
    "      return math.log(x)+1\n",
    "    return 0\n",
    "     \n",
    "print('                                  *** weighted term freq for each  term in doc*****                                              ')\n",
    "print()        \n",
    "for i in range(1,len(document_of_terms)+1):\n",
    "    term_freq['doc'+str(i)]= term_freq['doc'+str(i)].apply(get_weighted_term_freq) \n",
    "    \n",
    "print(term_freq,'\\n')\n",
    "print()\n",
    " \n",
    "#idf and tf*idf\n",
    " \n",
    "tfd=pd.DataFrame(columns=['df','idf'])\n",
    "\n",
    "print('                                  ***        IDF          ***                                              ')\n",
    "for i in range(len(term_freq)):\n",
    "    \n",
    "    frequancy=term_freq.iloc[i].values.sum()\n",
    "    \n",
    "    tfd.loc[i,'df']=frequancy\n",
    "    tfd.loc[i,'idf']=math.log10(10/(float(frequancy)))\n",
    "    \n",
    "tfd.index=term_freq.index\n",
    "    \n",
    "print(tfd)\n",
    "\n",
    "# tf*idf\n",
    "\n",
    "tf_idf=term_freq.multiply(tfd['idf'],axis=0)\n",
    "\n",
    "print('                                  ***         TF*IDF             ***                                              ')\n",
    "print()      \n",
    "print(tf_idf)\n",
    "\n",
    "\n",
    "# doc length\n",
    "\n",
    "def get_doc_len(col):\n",
    "    return np.sqrt(tf_idf[col].apply(lambda x:x**2).sum())\n",
    "\n",
    "doc_len=pd.DataFrame()\n",
    "for col in tf_idf.columns:\n",
    "    doc_len.loc['length', col+'__length']=get_doc_len(col)   \n",
    "print('                                  ***         Doc Length             ***                                              ') \n",
    "print(doc_len,'\\n')    \n",
    "\n",
    "\n",
    "\n",
    "# normalization\n",
    "norm_tf_idf= tf_idf.divide(doc_len.values[0],axis=1)\n",
    "print('                                  ***        Normalized TF*IDF           ***                                              ')\n",
    "print()\n",
    "print(norm_tf_idf)\n",
    "\n",
    "\n",
    "# Handling boolean operators\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "boolean_operators = ['and', 'or', 'not']\n",
    "boolean_operator = None\n",
    "\n",
    "for op in boolean_operators:\n",
    "    if op in document_of_terms:\n",
    "        boolean_operator = op\n",
    "        break\n",
    "\n",
    "matched_documents = None\n",
    "\n",
    "# Split the query based on the boolean operator\n",
    "query_parts = query.lower().split(boolean_operator)\n",
    "# Tokenization and stemming for each part\n",
    "\"\"\"query_parts_terms = [\n",
    "    #[PorterStemmer.stem(terms) for term in document_of_terms(term)] for term in query_parts\n",
    "    [document_of_stemms(stemmed_terms)] for stemmed_terms in query_parts\n",
    "]\"\"\"\n",
    "query_parts_terms = [stemmer.stem(term) for term in query_parts]\n",
    "\n",
    "print(query_parts_terms)\n",
    "\n",
    "if boolean_operator:\n",
    "    # Find matched documents for each part\n",
    "    matched_documents_parts = []\n",
    "    for part_terms in query_parts_terms:\n",
    "        matched_documents_part = set(range(1, len(document_lengths) + 1))  # Initialize with all documents\n",
    "        for term in part_terms:\n",
    "            if term in positional_index:\n",
    "                matched_documents_part &= set(positional_index[term]['docs'].keys()) # type: ignore\n",
    "        matched_documents_parts.append(matched_documents_part)\n",
    "    print(matched_documents_parts)\n",
    "    # Apply boolean operator\n",
    "    if boolean_operator == 'and':\n",
    "        matched_documents = set.intersection(*matched_documents_parts)\n",
    "    elif boolean_operator == 'or':\n",
    "        matched_documents = set.union(*matched_documents_parts)\n",
    "    elif boolean_operator == 'not':\n",
    "        matched_documents = set.difference(set(range(1, len(document_lengths) + 1)), *matched_documents_parts)\n",
    "    \n",
    "else:\n",
    "    # No boolean operator, find matched documents without considering boolean logic\n",
    "    matched_documents = set(range(1, len(document_lengths) + 1))\n",
    "    for term in document_of_terms:\n",
    "        if term in positional_index:\n",
    "            matched_documents &= set(positional_index[term]['docs'].keys()) # type: ignore\n",
    "\n",
    "print(\"Matched Documents:\", matched_documents)\n",
    "\n",
    "\n",
    "# Get distinct terms in the query\n",
    "document_of_terms = [item for sublist in query_parts_terms for item in sublist]\n",
    "original_document_of_terms = document_of_terms\n",
    "document_of_terms = list(set(document_of_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from nltk.tokenize import word_tokenize\n",
    "from natsort import natsorted\n",
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "#preprocessing\n",
    "       \n",
    "files_name=natsorted(os.listdir('Files'))\n",
    "document_of_terms=[]\n",
    "for files in files_name:\n",
    "    with open(f'Files/{files}','r') as f:\n",
    "        document=f.read()\n",
    "        \n",
    "         \n",
    "    tokenized_documents=word_tokenize(document)\n",
    "    terms=[]\n",
    "    for word in tokenized_documents:\n",
    "            terms.append(word)\n",
    "    document_of_terms.append(terms)\n",
    "\n",
    "print('                                               ***    Documents in  Terms   ***                                                                        \\n    '\n",
    "    ,  document_of_terms,'\\n')                        \n",
    "\n",
    "#stemming \n",
    "stemmer=PorterStemmer()\n",
    "document_of_stemms=[]\n",
    "for terms in document_of_terms:\n",
    "     for word in terms :\n",
    "         stemmed_terms=[stemmer.stem(word)]\n",
    "         document_of_stemms.append(stemmed_terms)\n",
    "\n",
    "print('                                               ***     Stemmed Terms   ***                                                                        \\n    '\n",
    "    ,  document_of_stemms,'\\n')        \n",
    "\n",
    "#positional_index\n",
    "\n",
    "document_number = 0\n",
    "positional_index = {}\n",
    "\n",
    "print() \n",
    "print(\"                                               ***     positions    ***                                       \\n   \")\n",
    "for document in document_of_terms:\n",
    "    for positional, term in enumerate(document):\n",
    "        if term in positional_index:\n",
    "            positional_index[term][0] = positional_index[term][0] + 1\n",
    "\n",
    "            if document_number in positional_index[term][1]:\n",
    "                positional_index[term][1][document_number].append(positional)\n",
    "            else:\n",
    "                positional_index[term][1][document_number] = [positional]\n",
    "\n",
    "        else:\n",
    "            positional_index[term] = []\n",
    "            positional_index[term].append(1)\n",
    "            positional_index[term].append({})\n",
    "            positional_index[term][1][document_number] = [positional]\n",
    "\n",
    "    document_number += 1\n",
    "    \n",
    "\n",
    "print(positional_index,'\\n')\n",
    "\n",
    "\n",
    "#query_preprocessing\n",
    "\n",
    "query=input('put query here:')\n",
    "\n",
    "final_list=[[]for i in range (10)]\n",
    "print('                                    *** all response list ***                                                     ')\n",
    "for word in query.split():    \n",
    "    if word in positional_index.keys():\n",
    "        for key in positional_index[word][1].keys():\n",
    "            \n",
    "            if final_list[key]!=[]:\n",
    "                \n",
    "                 if final_list[key][-1] == positional_index[word][1][key][0]-1:\n",
    "                        final_list[key].append(positional_index[word][1][key][0])\n",
    "        \n",
    "            else:\n",
    "                 final_list[key].append(positional_index[word][1][key][0])\n",
    "            \n",
    "print(final_list,'\\n')\n",
    "\n",
    "print('                                  ***response list to query***                                              ')\n",
    "for position,list in enumerate(final_list,start=1):\n",
    "    if len(list)==len(query.split()):\n",
    "      print(position,list)     \n",
    "\n",
    "\n",
    "\n",
    "#find tf and wtf\n",
    "\n",
    "\n",
    "\n",
    "all_words=[]\n",
    "for doc in document_of_terms:\n",
    "    for word in doc:\n",
    "        all_words.append(word)\n",
    " #tf       \n",
    "def get_term_freq(doc):\n",
    "    words_found=dict.fromkeys(all_words,0)\n",
    "    for word in doc:\n",
    "        words_found[word]+=1\n",
    "    return words_found\n",
    "\n",
    "term_freq=pd.DataFrame(get_term_freq(document_of_terms[0]).values(),index=get_term_freq(document_of_terms[0]).keys())              \n",
    "print('                                  ***term freq for each  term in doc***                                              ')\n",
    "print()        \n",
    "for i in range(1,len(document_of_terms)):\n",
    "    term_freq[i]=get_term_freq(document_of_terms[i]).values()\n",
    "    \n",
    "term_freq.columns=['doc'+str(i) for i in range (1,12)]    \n",
    "print(term_freq)     \n",
    "\n",
    "#wtf\n",
    "\n",
    "def get_weighted_term_freq(x):\n",
    "    if x>0:\n",
    "      return math.log(x)+1\n",
    "    return 0\n",
    "     \n",
    "print('                                  *** weighted term freq for each  term in doc*****                                              ')\n",
    "print()        \n",
    "for i in range(1,len(document_of_terms)+1):\n",
    "    term_freq['doc'+str(i)]= term_freq['doc'+str(i)].apply(get_weighted_term_freq) \n",
    "    \n",
    "print(term_freq,'\\n')\n",
    "print()\n",
    " \n",
    "#idf and tf*idf\n",
    " \n",
    "tfd=pd.DataFrame(columns=['df','idf'])\n",
    "\n",
    "print('                                  ***        IDF          ***                                              ')\n",
    "for i in range(len(term_freq)):\n",
    "    \n",
    "    frequancy=term_freq.iloc[i].values.sum()\n",
    "    \n",
    "    tfd.loc[i,'df']=frequancy\n",
    "    tfd.loc[i,'idf']=math.log10(10/(float(frequancy)))\n",
    "    \n",
    "tfd.index=term_freq.index\n",
    "    \n",
    "print(tfd)\n",
    "\n",
    "# tf*idf\n",
    "\n",
    "tf_idf=term_freq.multiply(tfd['idf'],axis=0)\n",
    "\n",
    "print('                                  ***         TF*IDF             ***                                              ')\n",
    "print()      \n",
    "print(tf_idf)\n",
    "\n",
    "\n",
    "# doc length\n",
    "\n",
    "def get_doc_len(col):\n",
    "    return np.sqrt(tf_idf[col].apply(lambda x:x**2).sum())\n",
    "\n",
    "doc_len=pd.DataFrame()\n",
    "for col in tf_idf.columns:\n",
    "    doc_len.loc['length', col+'__length']=get_doc_len(col)   \n",
    "print('                                  ***         Doc Length             ***                                              ') \n",
    "print(doc_len,'\\n')    \n",
    "\n",
    "\n",
    "\n",
    "# normalization\n",
    "norm_tf_idf= tf_idf.divide(doc_len.values[0],axis=1)\n",
    "print('                                  ***        Normalized TF*IDF           ***                                              ')\n",
    "print()\n",
    "print(norm_tf_idf)\n",
    "\n",
    "\n",
    "# Handling boolean operators\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "boolean_operators = ['and', 'or', 'not']\n",
    "boolean_operator = None\n",
    "\n",
    "for op in boolean_operators:\n",
    "    if op in query:\n",
    "        boolean_operator = op\n",
    "        break\n",
    "\n",
    "matched_documents = None\n",
    "\n",
    "# Split the query based on the boolean operator\n",
    "query_parts = query.lower().split(boolean_operator)\n",
    "# Tokenization and stemming for each part\n",
    "\"\"\"query_parts_terms = [\n",
    "    #[PorterStemmer.stem(terms) for term in document_of_terms(term)] for term in query_parts\n",
    "    [document_of_stemms(stemmed_terms)] for stemmed_terms in query_parts\n",
    "]\"\"\"\n",
    "query_parts_terms = [stemmer.stem(term) for term in query_parts]\n",
    "\n",
    "print(query_parts_terms)\n",
    "\n",
    "if boolean_operator:\n",
    "    # Find matched documents for each part\n",
    "    matched_documents_parts = []\n",
    "    for part_terms in query_parts_terms:\n",
    "        matched_documents_part = set(range(1, len(doc_len) + 1))  # Initialize with all documents\n",
    "        for term in part_terms:\n",
    "            if term in positional_index:\n",
    "                matched_documents_part &= set(positional_index[term][1].keys()) # type: ignore\n",
    "        matched_documents_parts.append(matched_documents_part)\n",
    "    print(matched_documents_parts)\n",
    "    # Apply boolean operator\n",
    "    if boolean_operator == 'and':\n",
    "        matched_documents = set.intersection(*matched_documents_parts)\n",
    "    elif boolean_operator == 'or':\n",
    "        matched_documents = set.union(*matched_documents_parts)\n",
    "    elif boolean_operator == 'not':\n",
    "        matched_documents = set.difference(set(range(1, len(doc_len) + 1)), *matched_documents_parts)\n",
    "    \n",
    "else:\n",
    "    # No boolean operator, find matched documents without considering boolean logic\n",
    "    matched_documents = set(range(1, len(doc_len) + 1))\n",
    "\n",
    "for term_list in document_of_terms:\n",
    "    for term in term_list:\n",
    "        if term in positional_index:\n",
    "            matched_documents &= set(positional_index[term][1].keys())\n",
    "\n",
    "print(\"Matched Documents:\", matched_documents)\n",
    "\n",
    "\n",
    "# Get distinct terms in the query\n",
    "\"\"\"document_of_terms = [item for sublist in query_parts_terms for item in sublist]\n",
    "original_document_of_terms = document_of_terms\n",
    "document_of_terms = list(set(document_of_terms))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from nltk.tokenize import word_tokenize\n",
    "from natsort import natsorted\n",
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "#preprocessing\n",
    "       \n",
    "files_name=natsorted(os.listdir('Files'))\n",
    "document_of_terms=[]\n",
    "for files in files_name:\n",
    "    with open(f'Files/{files}','r') as f:\n",
    "        document=f.read()\n",
    "        \n",
    "         \n",
    "    tokenized_documents=word_tokenize(document)\n",
    "    terms=[]\n",
    "    for word in tokenized_documents:\n",
    "            terms.append(word)\n",
    "    document_of_terms.append(terms)\n",
    "\n",
    "print('                                               ***    Documents in  Terms   ***                                                                        \\n    '\n",
    "    ,  document_of_terms,'\\n')                        \n",
    "\n",
    "#stemming \n",
    "stemmer=PorterStemmer()\n",
    "document_of_stemms=[]\n",
    "for terms in document_of_terms:\n",
    "     for word in terms :\n",
    "         stemmed_terms=[stemmer.stem(word)]\n",
    "         document_of_stemms.append(stemmed_terms)\n",
    "\n",
    "print('                                               ***     Stemmed Terms   ***                                                                        \\n    '\n",
    "    ,  document_of_stemms,'\\n')        \n",
    "\n",
    "#positional_index\n",
    "\n",
    "document_number = 0\n",
    "positional_index = {}\n",
    "\n",
    "print() \n",
    "print(\"                                               ***     positions    ***                                       \\n   \")\n",
    "for document in document_of_terms:\n",
    "    for positional, term in enumerate(document):\n",
    "        if term in positional_index:\n",
    "            positional_index[term][0] = positional_index[term][0] + 1\n",
    "\n",
    "            if document_number in positional_index[term][1]:\n",
    "                positional_index[term][1][document_number].append(positional)\n",
    "            else:\n",
    "                positional_index[term][1][document_number] = [positional]\n",
    "\n",
    "        else:\n",
    "            positional_index[term] = []\n",
    "            positional_index[term].append(1)\n",
    "            positional_index[term].append({})\n",
    "            positional_index[term][1][document_number] = [positional]\n",
    "\n",
    "    document_number += 1\n",
    "    \n",
    "\n",
    "print(positional_index,'\\n')\n",
    "\n",
    "\n",
    "#query_preprocessing\n",
    "\n",
    "query=input('put query here:')\n",
    "\n",
    "final_list=[[]for i in range (10)]\n",
    "print('                                    *** all response list ***                                                     ')\n",
    "for word in query.split():    \n",
    "    if word in positional_index.keys():\n",
    "        for key in positional_index[word][1].keys():\n",
    "            \n",
    "            if final_list[key]!=[]:\n",
    "                \n",
    "                 if final_list[key][-1] == positional_index[word][1][key][0]-1:\n",
    "                        final_list[key].append(positional_index[word][1][key][0])\n",
    "        \n",
    "            else:\n",
    "                 final_list[key].append(positional_index[word][1][key][0])\n",
    "            \n",
    "print(final_list,'\\n')\n",
    "\n",
    "print('                                  ***response list to query***                                              ')\n",
    "for position,list in enumerate(final_list,start=1):\n",
    "    if len(list)==len(query.split()):\n",
    "      print(position,list)     \n",
    "\n",
    "\n",
    "\n",
    "#find tf and wtf\n",
    "\n",
    "\n",
    "\n",
    "all_words=[]\n",
    "for doc in document_of_terms:\n",
    "    for word in doc:\n",
    "        all_words.append(word)\n",
    " #tf       \n",
    "def get_term_freq(doc):\n",
    "    words_found=dict.fromkeys(all_words,0)\n",
    "    for word in doc:\n",
    "        words_found[word]+=1\n",
    "    return words_found\n",
    "\n",
    "term_freq=pd.DataFrame(get_term_freq(document_of_terms[0]).values(),index=get_term_freq(document_of_terms[0]).keys())              \n",
    "print('                                  ***term freq for each  term in doc***                                              ')\n",
    "print()        \n",
    "for i in range(1,len(document_of_terms)):\n",
    "    term_freq[i]=get_term_freq(document_of_terms[i]).values()\n",
    "    \n",
    "term_freq.columns=['doc'+str(i) for i in range (1,12)]    \n",
    "print(term_freq)     \n",
    "\n",
    "#wtf\n",
    "\n",
    "def get_weighted_term_freq(x):\n",
    "    if x>0:\n",
    "      return math.log(x)+1\n",
    "    return 0\n",
    "     \n",
    "print('                                  *** weighted term freq for each  term in doc*****                                              ')\n",
    "print()        \n",
    "for i in range(1,len(document_of_terms)+1):\n",
    "    term_freq['doc'+str(i)]= term_freq['doc'+str(i)].apply(get_weighted_term_freq) \n",
    "    \n",
    "print(term_freq,'\\n')\n",
    "print()\n",
    " \n",
    "#idf and tf*idf\n",
    " \n",
    "tfd=pd.DataFrame(columns=['df','idf'])\n",
    "\n",
    "print('                                  ***        IDF          ***                                              ')\n",
    "for i in range(len(term_freq)):\n",
    "    \n",
    "    frequancy=term_freq.iloc[i].values.sum()\n",
    "    \n",
    "    tfd.loc[i,'df']=frequancy\n",
    "    tfd.loc[i,'idf']=math.log10(10/(float(frequancy)))\n",
    "    \n",
    "tfd.index=term_freq.index\n",
    "    \n",
    "print(tfd)\n",
    "\n",
    "# tf*idf\n",
    "\n",
    "tf_idf=term_freq.multiply(tfd['idf'],axis=0)\n",
    "\n",
    "print('                                  ***         TF*IDF             ***                                              ')\n",
    "print()      \n",
    "print(tf_idf)\n",
    "\n",
    "\n",
    "# doc length\n",
    "\n",
    "def get_doc_len(col):\n",
    "    return np.sqrt(tf_idf[col].apply(lambda x:x**2).sum())\n",
    "\n",
    "doc_len=pd.DataFrame()\n",
    "for col in tf_idf.columns:\n",
    "    doc_len.loc['length', col+'__length']=get_doc_len(col)   \n",
    "print('                                  ***         Doc Length             ***                                              ') \n",
    "print(doc_len,'\\n')    \n",
    "\n",
    "\n",
    "\n",
    "# normalization\n",
    "norm_tf_idf= tf_idf.divide(doc_len.values[0],axis=1)\n",
    "print('                                  ***        Normalized TF*IDF           ***                                              ')\n",
    "print()\n",
    "print(norm_tf_idf)\n",
    "\n",
    "\n",
    "# Handling boolean operators\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "boolean_operators = ['and', 'or', 'not']\n",
    "boolean_operator = None\n",
    "\n",
    "for op in boolean_operators:\n",
    "    if op in query:\n",
    "        boolean_operator = op\n",
    "        break\n",
    "\n",
    "matched_documents = None\n",
    "\n",
    "# Split the query based on the boolean operator\n",
    "query_parts = query.lower().split(boolean_operator)\n",
    "# Tokenization and stemming for each part\n",
    "\n",
    "\n",
    "\n",
    "#print(document_of_terms)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"if boolean_operator:\n",
    "    # Find matched documents for each part\n",
    "    matched_documents_parts = []\n",
    "    i = 0\n",
    "    for part_terms in document_of_terms:\n",
    "        matched_documents_part = set(range(1, len(doc_len) + 1))  # Initialize with all documents\n",
    "        for term in part_terms:\n",
    "            if term in positional_index:\n",
    "                matched_documents_part &= set(positional_index[term][i].keys()) # type: ignore\n",
    "                i=i+1\n",
    "        matched_documents_parts.append(matched_documents_part)\n",
    "    print(matched_documents_parts)\n",
    "    # Apply boolean operator\n",
    "    if boolean_operator == 'and':\n",
    "        matched_documents = set.intersection(*matched_documents_parts)\n",
    "    elif boolean_operator == 'or':\n",
    "        matched_documents = set.union(*matched_documents_parts)\n",
    "    elif boolean_operator == 'not':\n",
    "        matched_documents = set.difference(set(range(1, len(doc_len) + 1)), *matched_documents_parts)\n",
    "    \n",
    "else:\n",
    "    # No boolean operator, find matched documents without considering boolean logic\n",
    "    matched_documents = set(range(1, len(doc_len) + 1))\n",
    "\n",
    "for term_list in document_of_terms:\n",
    "    for term in term_list:\n",
    "        if term in positional_index:\n",
    "            matched_documents &= set(positional_index[term][i].keys())\n",
    "            i=i+1\n",
    "print(\"Matched Documents:\", matched_documents)\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
