{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from natsort import natsorted\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "#preprocessing\n",
    "       \n",
    "files_name=natsorted(os.listdir('files'))\n",
    "document_of_terms=[]\n",
    "for files in files_name:\n",
    "    with open(f'files/{files}','r') as f:\n",
    "        document=f.read()\n",
    "        \n",
    "         \n",
    "    tokenized_documents=word_tokenize(document)\n",
    "    terms=[]\n",
    "    for word in tokenized_documents:\n",
    "            terms.append(word)\n",
    "    document_of_terms.append(terms)\n",
    "\n",
    "print('                                               *****    Documents in  Terms   *****                                                                        \\n    '\n",
    "    ,  document_of_terms,'\\n')                        \n",
    "\n",
    "#stemming \n",
    "stemmer=PorterStemmer()\n",
    "document_of_stemms=[]\n",
    "for terms in document_of_terms:\n",
    "     for word in terms :\n",
    "         stemmed_terms=[stemmer.stem(word)]\n",
    "         document_of_stemms.append(stemmed_terms)\n",
    "\n",
    "print('                                               *****     Stemmed Terms   *****                                                                        \\n    '\n",
    "    ,  document_of_stemms,'\\n')        \n",
    "\n",
    "#positional_index\n",
    "\n",
    "document_number = 0\n",
    "positional_index = {}\n",
    "\n",
    "print() \n",
    "print(\"                                               *****     positions    *****                                       \\n   \")\n",
    "for document in document_of_terms:\n",
    "    for positional, term in enumerate(document):\n",
    "        if term in positional_index:\n",
    "            positional_index[term][0] = positional_index[term][0] + 1\n",
    "\n",
    "            if document_number in positional_index[term][1]:\n",
    "                positional_index[term][1][document_number].append(positional)\n",
    "            else:\n",
    "                positional_index[term][1][document_number] = [positional]\n",
    "\n",
    "        else:\n",
    "            positional_index[term] = []\n",
    "            positional_index[term].append(1)\n",
    "            positional_index[term].append({})\n",
    "            positional_index[term][1][document_number] = [positional]\n",
    "\n",
    "    document_number += 1\n",
    "    \n",
    "\n",
    "print(positional_index,'\\n')\n",
    "\n",
    "\n",
    "#query_preprocessing\n",
    "\n",
    "query=input('put query here:')\n",
    "\n",
    "final_list=[[]for i in range (10)]\n",
    "print('                                    ***** all response list *****                                                     ')\n",
    "for word in query.split():    \n",
    "    if word in positional_index.keys():\n",
    "        for key in positional_index[word][1].keys():\n",
    "            \n",
    "            if final_list[key]!=[]:\n",
    "                \n",
    "                 if final_list[key][-1] == positional_index[word][1][key][0]-1:\n",
    "                        final_list[key].append(positional_index[word][1][key][0])\n",
    "        \n",
    "            else:\n",
    "                 final_list[key].append(positional_index[word][1][key][0])\n",
    "            \n",
    "print(final_list,'\\n')\n",
    "\n",
    "print('                                  *****response list to query*****                                              ')\n",
    "for position,list in enumerate(final_list,start=1):\n",
    "    if len(list)==len(query.split()):\n",
    "      print(position,list)     \n",
    "\n",
    "\n",
    "\n",
    "#find tf and wtf\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "\n",
    "all_words=[]\n",
    "for doc in document_of_terms:\n",
    "    for word in doc:\n",
    "        all_words.append(word)\n",
    " #tf       \n",
    "def get_term_freq(doc):\n",
    "    words_found=dict.fromkeys(all_words,0)\n",
    "    for word in doc:\n",
    "        words_found[word]+=1\n",
    "    return words_found\n",
    "\n",
    "term_freq=pd.DataFrame(get_term_freq(document_of_terms[0]).values(),index=get_term_freq(document_of_terms[0]).keys())              \n",
    "print('                                  *****term freq for each  term in doc*****                                              ')\n",
    "print()        \n",
    "for i in range(1,len(document_of_terms)):\n",
    "    term_freq[i]=get_term_freq(document_of_terms[i]).values()\n",
    "    \n",
    "term_freq.columns=['doc'+str(i) for i in range (1,11)]    \n",
    "print(term_freq)     \n",
    "\n",
    "#wtf\n",
    "\n",
    "def get_weighted_term_freq(x):\n",
    "    if x>0:\n",
    "      return math.log(x)+1\n",
    "    return 0\n",
    "     \n",
    "print('                                  ***** weighted term freq for each  term in doc*****                                              ')\n",
    "print()        \n",
    "for i in range(1,len(document_of_terms)+1):\n",
    "    term_freq['doc'+str(i)]= term_freq['doc'+str(i)].apply(get_weighted_term_freq) \n",
    "    \n",
    "print(term_freq,'\\n')\n",
    "print()\n",
    " \n",
    "#idf and tf*idf\n",
    " \n",
    "tfd=pd.DataFrame(columns=['df','idf'])\n",
    "\n",
    "print('                                  *****        IDF          *****                                              ')\n",
    "for i in range(len(term_freq)):\n",
    "    \n",
    "    frequancy=term_freq.iloc[i].values.sum()\n",
    "    \n",
    "    tfd.loc[i,'df']=frequancy\n",
    "    tfd.loc[i,'idf']=math.log10(10/(float(frequancy)))\n",
    "    \n",
    "tfd.index=term_freq.index\n",
    "    \n",
    "print(tfd)\n",
    "\n",
    "# tf*idf\n",
    "\n",
    "tf_idf=term_freq.multiply(tfd['idf'],axis=0)\n",
    "\n",
    "print('                                  *****         TF*IDF             *****                                              ')\n",
    "print()      \n",
    "print(tf_idf)\n",
    "\n",
    "\n",
    "# doc length\n",
    "\n",
    "def get_doc_len(col):\n",
    "    return np.sqrt(tf_idf[col].apply(lambda x:x**2).sum())\n",
    "\n",
    "doc_len=pd.DataFrame()\n",
    "for col in tf_idf.columns:\n",
    "    doc_len.loc['length', col+'__length']=get_doc_len(col)   \n",
    "print('                                  *****         Doc Length             *****                                              ') \n",
    "print(doc_len,'\\n')    \n",
    "\n",
    "\n",
    "\n",
    "# normalization\n",
    "norm_tf_idf= tf_idf.divide(doc_len.values[0],axis=1)\n",
    "print('                                  *****        Normalized TF*IDF           *****                                              ')\n",
    "print()\n",
    "print(norm_tf_idf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# positional index median\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postfix(infix_tokens):\n",
    " \n",
    " #precendence initialization\n",
    " precedence = {}\n",
    " precedence['NOT'] = 3\n",
    " precedence['AND'] = 2\n",
    " precedence['OR'] = 1\n",
    " precedence['('] = 0\n",
    " precedence[')'] = 0\n",
    " output = []\n",
    " operator_stack = []\n",
    " \n",
    " #creating postfix expression\n",
    " for token in infix_tokens:\n",
    "    if (token == '('):\n",
    "        operator_stack.append(token)\n",
    "    elif (token == ')'):\n",
    "        operator = operator_stack.pop()\n",
    "        while (operator != '('):\n",
    "                 output.append(operator)\n",
    "                 operator = operator_stack.pop()\n",
    " \n",
    "    elif (token in precedence):\n",
    "        if (operator_stack):\n",
    "             current_operator = operator_stack[-1]\n",
    "             while (operator_stack and precedence[current_operator] > precedence[token]):\n",
    "                output.append(operator_stack.pop())\n",
    "                    if (operator_stack):\n",
    "                        current_operator = operator_stack[-1]\n",
    "                        operator_stack.append(token)\n",
    "                    else:\n",
    "                            output.append(token.lower())\n",
    " \n",
    " #while staack is not empty appending\n",
    "while (operator_stack):\n",
    "      output.append(operator_stack.pop())\n",
    "return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(q,dictionary_inverted):\n",
    "  q = q.replace('(', '( ')\n",
    "  q = q.replace(')', ' )')\n",
    "  q = q.split('' )\n",
    "  query = []\n",
    "for i in q:\n",
    "    query.append(ps.stem(i))\n",
    "        for i in range(0,len(query)):\n",
    "             if ( query[i]== 'and' or query[i]== 'or' or query[i]== 'not'):\n",
    "                 query[i] = query[i].upper()\n",
    "                 results_stack = []\n",
    "                 postfix_queue = postfix(query)\n",
    "         \n",
    "         #evaluating postfix query expression\n",
    "                     for i in postfix_queue:\n",
    "                          i = i.replace('(', '')\n",
    "                           i = i.replace(')', '')\n",
    "                          i = i.lower()\n",
    "                          i = dictionary_inverted.get(i)\n",
    "                             results_stack.append(i)\n",
    "             \n",
    "             elif (i=='AND'):\n",
    "                 a = results_stack.pop()\n",
    "                 b = results_stack.pop()\n",
    "                 results_stack.append(AND_op(a,b))\n",
    "             elif (i=='OR'):\n",
    "                a = results_stack.pop()\n",
    "                b = results_stack.pop()\n",
    "                results_stack.append(OR_op(a,b))\n",
    "             elif (i == 'NOT'):\n",
    "                a = results_stack.pop()\n",
    "                print(a)\n",
    "                results_stack.append(NOT_op(a))\n",
    " \n",
    " return results_stack.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AND two posting lists\n",
    "def AND_op(word1,word2):\n",
    "    if ((word1) and (word2)):\n",
    "       return list(word1).intersection(word2)\n",
    "    else:\n",
    "       return list()\n",
    " \n",
    "#OR two posting lists\n",
    "def OR_op(word1,word2):\n",
    " return list(word1).union(word2)\n",
    " \n",
    "#NOT two posting lists\n",
    "def NOT_op(a):\n",
    "   tot_docs = list(range(0,56))\n",
    " return list(tot_docs).symmetric_difference(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sol deliverd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling boolean operators\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from natsort import natsorted\n",
    "from nltk.stem import PorterStemmer as porter_stemmer\n",
    "\n",
    "\n",
    "files_name=natsorted(os.listdir('files'))\n",
    "document_of_terms=[]\n",
    "for files in files_name:\n",
    "    with open(f'files/{files}','r') as f:\n",
    "        document=f.read()\n",
    "        \n",
    "boolean_operators = ['and', 'or', 'not']\n",
    "boolean_operator = None\n",
    "\n",
    "for op in boolean_operators:\n",
    "    if op in document_of_terms:\n",
    "        boolean_operator = op\n",
    "        break\n",
    "\n",
    "matched_documents = None\n",
    "\n",
    "\n",
    "query=input('put query here:')\n",
    "# Split the query based on the boolean operator\n",
    "query_parts = query.lower().split(boolean_operator)\n",
    "# Tokenization and stemming for each part\n",
    "query_parts_terms = [\n",
    "    [porter_stemmer.stem(term) for term in document_of_terms(term)] for part in query_parts\n",
    "]\n",
    "print(query_parts_terms)\n",
    "\n",
    "if boolean_operator:\n",
    "    # Find matched documents for each part\n",
    "    matched_documents_parts = []\n",
    "    for part_terms in query_parts_terms:\n",
    "        matched_documents_part = set(range(1, len(document_lengths) + 1))  # Initialize with all documents\n",
    "        for term in part_terms:\n",
    "            if term in positional_index:\n",
    "                matched_documents_part &= set(positional_index[term]['docs'].keys()) # type: ignore\n",
    "        matched_documents_parts.append(matched_documents_part)\n",
    "    print(matched_documents_parts)\n",
    "    # Apply boolean operator\n",
    "    if boolean_operator == 'and':\n",
    "        matched_documents = set.intersection(*matched_documents_parts)\n",
    "    elif boolean_operator == 'or':\n",
    "        matched_documents = set.union(*matched_documents_parts)\n",
    "    elif boolean_operator == 'not':\n",
    "        matched_documents = set.difference(set(range(1, len(document_lengths) + 1)), *matched_documents_parts)\n",
    "    \n",
    "else:\n",
    "    # No boolean operator, find matched documents without considering boolean logic\n",
    "    matched_documents = set(range(1, len(document_lengths) + 1))\n",
    "    for term in document_of_terms:\n",
    "        if term in positional_index:\n",
    "            matched_documents &= set(positional_index[term]['docs'].keys()) # type: ignore\n",
    "\n",
    "print(\"Matched Documents:\", matched_documents)\n",
    "\n",
    "\n",
    "# Get distinct terms in the query\n",
    "document_of_terms = [item for sublist in query_parts_terms for item in sublist]\n",
    "original_document_of_terms = document_of_terms\n",
    "document_of_terms = list(set(document_of_terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bart of bool fun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(list1, list2):\n",
    "    return list(set(list1).intersection(list2))\n",
    "def union(list1, list2):\n",
    "    return list(set(list1).union(list2))\n",
    "def notin(list1, list2):\n",
    "    return [filter(lambda x: x not in list1, sublist) for sublist in list2]\n",
    "\n",
    "intersection(positional_index['fools'], intersection(positional_index['fear'], positional_index['angels']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
